# DataFlow Operator

DataFlow Operator - это Kubernetes оператор для потоковой передачи данных между различными источниками данных с поддержкой трансформаций сообщений.

## Обзор

DataFlow Operator позволяет декларативно определять потоки данных между различными источниками и приемниками через Kubernetes Custom Resource Definitions (CRD). Оператор автоматически управляет жизненным циклом потоков данных, обрабатывает сообщения и применяет необходимые трансформации.

## Основные возможности

### Поддержка множественных источников данных

- **Kafka** - чтение и запись сообщений из/в топики Kafka с поддержкой TLS и SASL аутентификации
- **PostgreSQL** - чтение из таблиц и запись в таблицы PostgreSQL с поддержкой кастомных SQL запросов и батч-вставок
- **Trino** - чтение из таблиц и запись в таблицы Trino с поддержкой SQL запросов, аутентификации Keycloak OAuth2 и батч-вставок

### Богатый набор трансформаций

- **Timestamp** - добавление временной метки к каждому сообщению
- **Flatten** - развертывание массивов в отдельные сообщения с сохранением родительских полей
- **Filter** - фильтрация сообщений на основе JSONPath условий
- **Mask** - маскирование чувствительных данных с сохранением длины или без
- **Router** - маршрутизация сообщений в разные приемники на основе условий
- **Select** - выбор определенных полей из сообщений
- **Remove** - удаление указанных полей из сообщений
- **SnakeCase** - преобразование имен полей в snake_case
- **CamelCase** - преобразование имен полей в CamelCase

### Гибкая маршрутизация

Оператор поддерживает условную маршрутизацию сообщений в разные приемники на основе JSONPath выражений, что позволяет создавать сложные сценарии обработки данных.

### Простое управление

Декларативная конфигурация через Kubernetes CRD позволяет легко управлять потоками данных, версионировать конфигурации и интегрироваться с CI/CD системами.

### Безопасная конфигурация

Поддержка конфигурации коннекторов из Kubernetes Secrets через `SecretRef` позволяет безопасно хранить credentials, токены и connection strings без их явного указания в спецификации DataFlow.

## Быстрый старт

### Установка оператора

```bash
# Установка оператора через Helm из OCI registry
helm install dataflow-operator oci://ghcr.io/ilyario/helm-charts/dataflow-operator

# Проверка установки
kubectl get pods -l app.kubernetes.io/name=dataflow-operator

# Проверка CRD
kubectl get crd dataflows.dataflow.dataflow.io
```

### Создание первого потока данных

Создайте простой поток данных из Kafka в PostgreSQL:

```bash
kubectl apply -f config/samples/kafka-to-postgres.yaml
```

Проверьте статус:

```bash
kubectl get dataflow kafka-to-postgres
kubectl describe dataflow kafka-to-postgres
```

### Локальная разработка

Для локальной разработки и тестирования:

```bash
# Запуск зависимостей (Kafka, PostgreSQL)
docker-compose up -d

# Запуск оператора локально
make run
```

## Архитектура

Оператор состоит из следующих компонентов:

### CRD (Custom Resource Definitions)

Определяет схему ресурса `DataFlow`, который описывает конфигурацию потока данных, включая источник, приемник и список трансформаций.

### Controller

Kubernetes контроллер, который отслеживает изменения ресурсов `DataFlow` и управляет их жизненным циклом. Контроллер создает и управляет процессорами для каждого активного потока данных.

### Connectors

Модульная система коннекторов для различных источников и приемников данных. Каждый коннектор реализует стандартный интерфейс для чтения или записи данных.

### Transformers

Модули трансформации сообщений, которые применяются последовательно к каждому сообщению в порядке, указанном в конфигурации.

### Processor

Оркестратор обработки сообщений, который координирует работу источника, трансформаций и приемника. Обрабатывает ошибки, ведет статистику и управляет жизненным циклом потока данных.

## Поддерживаемые источники и приемники

### Kafka

- Поддержка consumer groups для масштабирования
- TLS и SASL аутентификация
- Настройка начальной позиции чтения
- Поддержка ключей сообщений

### PostgreSQL

- Кастомные SQL запросы для источников
- Периодический опрос (polling) с настраиваемым интервалом
- Батч-вставки для повышения производительности
- Автоматическое создание таблиц
- Поддержка JSONB для гибкой схемы
- UPSERT режим для обновления существующих записей

### Trino

- Кастомные SQL запросы для источников
- Периодический опрос (polling) с настраиваемым интервалом
- Батч-вставки для повышения производительности
- Автоматическое создание таблиц
- Аутентификация через Keycloak OAuth2/OIDC

## Безопасность

### Kubernetes Secrets

Все коннекторы поддерживают конфигурацию из Kubernetes Secrets через поля `*SecretRef`:

- **Kafka**: brokers, topic, consumerGroup, SASL credentials, TLS сертификаты
- **PostgreSQL**: connectionString, table
- **Trino**: serverURL, catalog, schema, table, Keycloak credentials

Это позволяет:
- Безопасно хранить чувствительные данные
- Централизованно управлять credentials
- Ротировать secrets без изменения DataFlow ресурсов
- Контролировать доступ через Kubernetes RBAC

Подробнее см. раздел [Использование Secrets в Kubernetes](connectors.md#использование-secrets-в-kubernetes).

## Трансформации

Все трансформации поддерживают JSONPath для работы с вложенными структурами данных.

### Timestamp

Добавляет поле с временной меткой в формате RFC3339 или кастомном формате.

### Flatten

Развертывает массивы в отдельные сообщения, сохраняя все родительские поля. Полезно для обработки вложенных структур.

### Filter

Фильтрует сообщения на основе JSONPath условий. Поддерживает сложные логические выражения.

### Mask

Маскирует чувствительные данные, поддерживая сохранение длины или полное замещение символами.

### Router

Маршрутизирует сообщения в разные приемники на основе условий. Позволяет создавать сложные сценарии обработки.

### Select

Выбирает только указанные поля из сообщений, уменьшая размер данных и повышая производительность.

### Remove

Удаляет указанные поля из сообщений, полезно для очистки данных перед отправкой.

### SnakeCase

Преобразует имена полей в snake_case формат. Поддерживает рекурсивное преобразование вложенных объектов.

### CamelCase

Преобразует имена полей в CamelCase формат. Поддерживает рекурсивное преобразование вложенных объектов.

## Мониторинг и статус

Каждый ресурс `DataFlow` имеет статус, который включает:

- **Phase** - текущая фаза потока данных (Running, Error, etc.)
- **Message** - дополнительная информация о статусе
- **LastProcessedTime** - время последнего обработанного сообщения
- **ProcessedCount** - количество обработанных сообщений
- **ErrorCount** - количество ошибок

Оператор также экспортирует метрики Prometheus для детального мониторинга:
- Количество полученных/отправленных сообщений по каждому манифесту
- Ошибки в коннекторах и трансформерах
- Время обработки сообщений и выполнения трансформеров
- Статус подключения коннекторов

Подробнее см. раздел [Metrics](metrics.md).

## Документация

- [Getting Started](getting-started.md) - подробное руководство по началу работы
- [Connectors](connectors.md) - детальное описание всех коннекторов
- [Transformations](transformations.md) - подробное описание трансформаций с примерами
- [Examples](examples.md) - практические примеры использования
- [Metrics](metrics.md) - метрики Prometheus и мониторинг
- [Development](development.md) - руководство для разработчиков

## Лицензия

Apache License 2.0



